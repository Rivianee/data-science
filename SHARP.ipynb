{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRFSIvA+xv//GniN/2BI8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rivianee/data-science/blob/master/SHARP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Título: Explicabilidade em Modelos de Aprendizado de Máquina com SHAP**\n",
        "\n",
        "Introdução:\n",
        "Os modelos de aprendizado de máquina, como algoritmos de inteligência artificial, estão sendo amplamente utilizados para tomar decisões e fazer previsões em várias áreas. No entanto, muitas vezes, esses modelos são considerados como \"caixas-pretas\" devido à sua complexidade interna. Isso significa que é difícil entender por que um modelo específico tomou uma determinada decisão ou fez uma previsão. A explicabilidade desses modelos é de extrema importância, especialmente em áreas críticas como saúde, finanças e justiça, onde é necessário entender e justificar as decisões tomadas.\n",
        "\n",
        "Nesse contexto, o SHAP (SHapley Additive exPlanations) é um modelo teórico que fornece uma abordagem unificada para explicar a saída de qualquer modelo de aprendizado de máquina. Ele conecta a alocação de crédito ótima com explicações locais usando os valores clássicos de Shapley da teoria dos jogos e suas extensões relacionadas."
      ],
      "metadata": {
        "id": "2Gh5hvRcwVw5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ye48XHvu46P"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Carregar o conjunto de dados Iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que é o SHAP:\n",
        "Imagine que você tem um jogo de adivinhação onde você precisa adivinhar o objeto que alguém está pensando fazendo perguntas. O SHAP é como uma estratégia especial para descobrir quais perguntas são mais importantes para adivinhar a resposta correta.\n",
        "\n",
        "Vamos dar um exemplo. Suponha que você está tentando adivinhar o animal de estimação de alguém. Você pode fazer perguntas como \"Ele tem pelos?\", \"Ele mia?\" ou \"Ele late?\". Dependendo das respostas, você pode eliminar possibilidades e chegar mais perto da resposta certa. O SHAP faz algo semelhante, mas em vez de perguntas, ele olha para características ou informações sobre algo para entender como elas afetam as decisões dos computadores."
      ],
      "metadata": {
        "id": "U8XxsBnmxRPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PASSOS BÁSICOS PARA USO DO SHARP\n",
        "\n",
        "Esses são os passos básicos para usar a biblioteca SHAP. Primeiro, carregamos os dados e treinamos um modelo. Em seguida, criamos um Explainer usando o modelo treinado e calculamos os valores SHAP. Por fim, podemos visualizar as explicações para amostras individuais usando o force_plot e ter uma visão geral das contribuições de cada recurso usando o summary_plot.\n",
        "\n",
        "Lembre-se de que a interpretação dos resultados do SHAP requer um entendimento do contexto do problema e dos recursos do conjunto de dados específico que está sendo usado."
      ],
      "metadata": {
        "id": "mhCUavQuycye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Treinar o modelo de classificação usando XGBoost\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "j2WdwCtavAU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o objeto Explainer\n",
        "explainer = shap.Explainer(model)"
      ],
      "metadata": {
        "id": "5deU5x9pvG2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular os valores SHAP\n",
        "shap_values = explainer.shap_values(X)"
      ],
      "metadata": {
        "id": "Iw4kfGP6vHVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar a explicação para uma amostra específica\n",
        "sample_index = 0  # Índice da amostra que você deseja explicar\n",
        "shap.force_plot(explainer.expected_value, shap_values[sample_index], X[sample_index])"
      ],
      "metadata": {
        "id": "OElWiUfxvPC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotar um gráfico resumindo as contribuições de cada recurso\n",
        "shap.summary_plot(shap_values, X)"
      ],
      "metadata": {
        "id": "Kbfeh5VHvR_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benefícios do SHAP:\n",
        "O SHAP oferece vários benefícios. Primeiro, ele ajuda a entender quais recursos (neste caso, as palavras-chave) foram mais importantes para a decisão do modelo. Isso permite identificar padrões relevantes e descobrir quais características têm maior impacto nas previsões ou decisões do modelo.\n",
        "\n",
        "Em segundo lugar, o SHAP fornece explicações locais, ou seja, explicações individuais para cada instância de entrada. Por exemplo, no caso dos e-mails classificados como spam, o SHAP pode mostrar quais palavras-chave específicas contribuíram para a classificação de cada e-mail como spam.\n",
        "\n",
        "Aplicações do SHAP:\n",
        "Além do exemplo do modelo de classificação de e-mails, o SHAP tem várias outras aplicações. Por exemplo, em um modelo de recomendação de filmes, o SHAP pode ajudar a entender quais características de um filme (como gênero, diretor ou ator principal) são mais importantes para a recomendação feita pelo modelo. Isso pode ajudar a personalizar ainda mais as recomendações para os usuários.\n",
        "\n",
        "Outra aplicação é em modelos de detecção de fraude em transações financeiras. O SHAP pode explicar quais características da transação (como valor, localização ou histórico do cliente) foram consideradas relevantes pelo modelo para classificar a transação como fraudulenta ou legítima.\n",
        "\n",
        "Conclusão:\n",
        "O SHAP é uma ferramenta poderosa para explicar modelos de aprendizado de máquina. Ele permite entender quais recursos são mais importantes para as previsões ou decisões dos modelos, fornecendo insights sobre o funcionamento interno desses modelos. Com o SHAP, é possível identificar possíveis viéses, justificar decisões e promover a transparência e a confiança em aplicações de aprendizado de máquina."
      ],
      "metadata": {
        "id": "b1vIKdFtyFXU"
      }
    }
  ]
}